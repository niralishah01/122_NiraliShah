{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SBnmjlZOlbI"
   },
   "source": [
    "### **LAB7**\n",
    "\n",
    "### **AIM: Replace manual version of Logistic Regression using Tensorflow based version on tweeter samples**\n",
    "\n",
    "Nirali Shah | CE122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "YiEK-LhWN_hS"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "88Wh4hfFPPr_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "XSTUaxEjQjgO"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "GLEpH4tfTTiN"
   },
   "outputs": [],
   "source": [
    "#process_tweet(): cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\n",
    "def process_tweet(tweet):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    " \n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "            \n",
    "            #############################################################\n",
    "            # 1 remove stopwords\n",
    "            if(word not in stopwords_english and word not in string.punctuation):\n",
    "              steam_word=stemmer.stem(word)\n",
    "              tweets_clean.append(steam_word)\n",
    "            # 3 stemming word\n",
    "            # 4 Add it to tweets_clean\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "jZoZaoGLTXVo"
   },
   "outputs": [],
   "source": [
    "def build_freqs(tweets, ys):\n",
    "\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "    # Start with an empty dictionary and populate it by looping over all tweets\n",
    "    # and over all processed words in each tweet.\n",
    "    freqs = {}\n",
    "\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            \n",
    "            #############################################################\n",
    "            #Update the count of pair if present, set it to 1 otherwise\n",
    "            if pair not in freqs:\n",
    "              freqs[pair]=0\n",
    "            freqs[pair]+=1\n",
    "\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmJgnlnbT2nQ"
   },
   "source": [
    "Prepare the data\n",
    "* The `twitter_samples` contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "utVbpi8vTmf1"
   },
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrJhCauWUKeh"
   },
   "source": [
    "Divide data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "szAiXOyeT8AL"
   },
   "outputs": [],
   "source": [
    "# combine positive and negative labels\n",
    "data=all_positive_tweets+all_negative_tweets\n",
    "la=np.append(np.ones((len(all_positive_tweets), 1)), np.zeros((len(all_negative_tweets), 1)), axis=0)\n",
    "train_x,test_x,train_y,test_y=train_test_split(data,la,test_size=0.30,random_state=121)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWqL8rmWUdsc"
   },
   "source": [
    "Create the frequency dictionary using build_freqs() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "AJjU0wHhUsPg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 10517\n"
     ]
    }
   ],
   "source": [
    "freqs = build_freqs(train_x,train_y)\n",
    "\n",
    "# check the output\n",
    "print(\"type(freqs) = \" + str(type(freqs)))\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJ1bcFaDU39D"
   },
   "source": [
    "* HERE, The `freqs` dictionary is the frequency dictionary that's being built. \n",
    "* The key is the tuple (word, label), such as (\"happy\",1) or (\"happy\",0).  The value stored for each key is the count of how many times the word \"happy\" was associated with a positive label, or how many times \"happy\" was associated with a negative label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ornbM4DvU_Ze"
   },
   "source": [
    "Process Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "W0Uncp95UwHD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of a positive tweet: \n",
      " @moontwink one of the best couples :(\n",
      "\n",
      "This is an example of the processed version of the tweet: \n",
      " ['one', 'best', 'coupl', ':(']\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "print('This is an example of a positive tweet: \\n', train_x[0])\n",
    "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "QDTVirLQnRH4"
   },
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    " \n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 2)) \n",
    "    \n",
    "    #bias term is set to 1\n",
    "    # x[0,0] = 1 \n",
    "        \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "        \n",
    "        # increment the word count for the positive label 1\n",
    "        #############################################################\n",
    "        if((word,1) in freqs):\n",
    "          x[0,0]+=freqs[word,1]\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        #############################################################\n",
    "        if((word,0) in freqs):\n",
    "          x[0,1]+=freqs[word,0]\n",
    "    \n",
    "    assert(x.shape == (1, 2))\n",
    "    return x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 146. 3333.]\n"
     ]
    }
   ],
   "source": [
    "# Check the function\n",
    "\n",
    "# test 1\n",
    "# test on training data\n",
    "tmp1 = extract_features(train_x[0], freqs)\n",
    "print(tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[137.  20.]\n"
     ]
    }
   ],
   "source": [
    "# test 2:\n",
    "# check for when the words are not in the freqs dictionary\n",
    "tmp2 = extract_features('happy', freqs)\n",
    "print(tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet):\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "      saver.restore(sess,save_path='TSession')\n",
    "      data_i=[]\n",
    "      for t in tweet:\n",
    "        data_i.append(extract_features(t,freqs))\n",
    "      data_i=np.asarray(data_i)\n",
    "      return sess.run(tf.nn.sigmoid(tf.add(tf.matmul(a=data_i,b=W,transpose_b=True),b)))\n",
    "    print(\"Fail\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=tf.Variable(np.random.randn(1),name=\"Bias\")\n",
    "W=tf.Variable(np.random.randn(1,2),name=\"Bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for t in train_x:\n",
    "  data.append(extract_features(t,freqs))\n",
    "data=np.asarray(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Sigmoid_6:0\", shape=(7000, 1), dtype=float64)\n",
      "Tensor(\"logistic_loss_3:0\", shape=(7000, 1), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "Y_hat = tf.nn.sigmoid(tf.add(tf.matmul(np.asarray(data), W,transpose_b=True), b)) \n",
    "print(Y_hat)\n",
    "ta=np.asarray(train_y)\n",
    "# Sigmoid Cross Entropy Cost Function \n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits( \n",
    "                    logits = Y_hat, labels = ta) \n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Optimizer \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-4,name=\"GradientDescent\").minimize(cost) \n",
    "  \n",
    "# Global Variables Initializer \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias [-0.73436824]\n",
      "Weight [[ 0.33027682 -0.12382835]]\n",
      "Accuracy 0.4962857142857143\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "  \n",
    "  sess.run(init)\n",
    "  print(\"Bias\",sess.run(b))\n",
    "  print(\"Weight\",sess.run(W))\n",
    "  for epoch in range(400):\n",
    "    sess.run(optimizer)\n",
    "    preds=sess.run(Y_hat)\n",
    "    acc=((preds==ta).sum())/len(train_y)\n",
    "    accu=[]\n",
    "    repoch=False\n",
    "    if repoch:\n",
    "      accu.append(acc)\n",
    "    if epoch % 1000 == 0:\n",
    "      print(\"Accuracy\",acc)\n",
    "    saved_path = saver.save(sess, 'TSession')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from TSession\n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] 3000\n"
     ]
    }
   ],
   "source": [
    "preds=predict_tweet(test_x)\n",
    "print(preds,len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(x,y):\n",
    "  if len(x)!=len(y):\n",
    "    print(\"dimensions are different\")\n",
    "    return\n",
    "  return ((x==y).sum())/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8986666666666666\n"
     ]
    }
   ],
   "source": [
    "print(calculate_accuracy(preds,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPaWkCdPrX9KnT56L4R3j/t",
   "name": "122_Lab7_01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
